{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s4xTmQKkrGR"
      },
      "source": [
        "# Language Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25b1VTmVkrGT"
      },
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/inner-LMNt/Self-GPT.git\n",
        "%cd Self-GPT\n",
        "\n",
        "# Install requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "!pwd\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8efwRtXEkrGU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "from models.LLM.config import Config\n",
        "from models.LLM.GPT import GPT\n",
        "from models.LLM.bigram import BigramLanguageModel\n",
        "from models.LLM.trigram import TrigramLanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IjWv-4WkrGV"
      },
      "outputs": [],
      "source": [
        "def load_json(file):\n",
        "    with open(file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def train_n_gram(n=2):\n",
        "    config = Config()\n",
        "    torch.manual_seed(config.seed)\n",
        "\n",
        "    model_name = \"new_model\"\n",
        "\n",
        "    if n == 2:\n",
        "        path = config.checkpoint_dir + f\"/bigram/{model_name}.pth\"\n",
        "        LLM = BigramLanguageModel().to(config.device)\n",
        "    else:\n",
        "        path = config.checkpoint_dir + f\"/trigram/{model_name}.pth\"\n",
        "        LLM = TrigramLanguageModel().to(config.device)\n",
        "\n",
        "    try:\n",
        "        LLM.load_state_dict(torch.load(path, map_location=config.device))\n",
        "        print(\"Model loaded successfully:\", path)\n",
        "        print(\"Training...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Training model from scratch...\")\n",
        "\n",
        "    data = load_json(config.data_dir + '/' + config.data_file)\n",
        "    data = torch.tensor(data, dtype=torch.long)\n",
        "\n",
        "    split = int(config.train_split * len(data))\n",
        "    train_data, val_data = data[:split], data[split:]\n",
        "\n",
        "    def generate_batch(train):\n",
        "        data = train_data if train else val_data\n",
        "        offsets = torch.randint(len(data) - config.context_len, (config.batch_size,))\n",
        "        inputs = torch.stack([data[i:i+config.context_len] for i in offsets])\n",
        "        targets = torch.stack([data[i+1:i+config.context_len+1] for i in offsets])\n",
        "        return inputs.to(config.device), targets.to(config.device)\n",
        "\n",
        "    start = time.time()\n",
        "    optimizer = torch.optim.Adam(LLM.parameters(), lr=config.learning_rate)\n",
        "    for epoch in range(config.num_epochs):\n",
        "        x, y = generate_batch(True)\n",
        "        logits, loss = LLM.forward(x, y)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward() # Gradients\n",
        "        optimizer.step() # Update weights\n",
        "        if epoch % 200 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "    print(\"\\n______________________________________________________\")\n",
        "    print(f\"Training loss: {loss.item()}, Validation loss: {LLM.forward(*generate_batch(False))[1].item()}\")\n",
        "    print(f\"Training time: {time.time() - start} seconds\")\n",
        "    print(\"______________________________________________________\\n\")\n",
        "\n",
        "    print(f\"Saving model to {path}\")\n",
        "    try:\n",
        "        torch.save(LLM.state_dict(), path)\n",
        "        print(\"Model saved successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {e}\")\n",
        "\n",
        "def train_gpt():\n",
        "    config = Config()\n",
        "    torch.manual_seed(config.seed)\n",
        "\n",
        "    model_name = \"GPT-mini\"\n",
        "    path = config.checkpoint_dir + f\"/gpt/{model_name}.pth\"\n",
        "    LLM = GPT().to(config.device)\n",
        "\n",
        "    try:\n",
        "        LLM.load_state_dict(torch.load(path, weights_only=True))\n",
        "        print(\"Model loaded successfully:\", path)\n",
        "        print(\"Training...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Training model from scratch...\")\n",
        "\n",
        "    data = load_json(config.data_dir + '/' + config.data_file)\n",
        "    data = torch.tensor(data, dtype=torch.long)\n",
        "\n",
        "    split = int(config.train_split * len(data))\n",
        "    train_data, val_data = data[:split], data[split:]\n",
        "\n",
        "    def generate_batch(train):\n",
        "        data = train_data if train else val_data\n",
        "        offsets = torch.randint(len(data) - config.context_len, (config.batch_size,))\n",
        "        inputs = torch.stack([data[i:i+config.context_len] for i in offsets])\n",
        "        targets = torch.stack([data[i+1:i+config.context_len+1] for i in offsets])\n",
        "        return inputs.to(config.device), targets.to(config.device)\n",
        "\n",
        "    start = time.time()\n",
        "    optimizer = torch.optim.Adam(LLM.parameters(), lr=config.learning_rate)\n",
        "    for epoch in range(config.num_epochs):\n",
        "        x, y = generate_batch(True)\n",
        "        logits, loss = LLM.forward(x, y)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward() # Gradients\n",
        "        optimizer.step() # Update weights\n",
        "        if epoch % 200 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "    print(\"\\n______________________________________________________\")\n",
        "    print(f\"Training loss: {loss.item()}, Validation loss: {LLM.forward(*generate_batch(False))[1].item()}\")\n",
        "    print(f\"Training time: {time.time() - start} seconds\")\n",
        "    print(\"______________________________________________________\\n\")\n",
        "\n",
        "    print(f\"Saving model to {path}\")\n",
        "    try:\n",
        "        torch.save(LLM.state_dict(), path)\n",
        "        print(\"Model saved successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyo_7Z1kkrGV"
      },
      "source": [
        "## Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVUUsLt9krGX"
      },
      "outputs": [],
      "source": [
        "train_n_gram(3) # or 3\n",
        "# train_gpt()\n",
        "print(\"\\nFinished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjD9J3-b2Gy0"
      },
      "source": [
        "## Download Trained Model\n",
        "\n",
        "After training, you can download the model using the code or just right-clicking the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEi14tSikrGX"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "config = Config()\n",
        "model_type = \"gpt\"\n",
        "model_name = \"GPT-mini\"\n",
        "\n",
        "# Adjust the path based on which model you trained\n",
        "path = config.checkpoint_dir + f\"/{model_type}/{model_name}.pth\"\n",
        "\n",
        "if os.path.exists(path):\n",
        "    files.download(path)\n",
        "else:\n",
        "    print(f\"Model file not found at {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Inference"
      ],
      "metadata": {
        "id": "Wyl-ZGYk2I-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from data.tokenizer import Tokenizer\n",
        "\n",
        "def gpt_inference(save=False):\n",
        "    config = Config()\n",
        "    tokenizer = Tokenizer()\n",
        "\n",
        "    LLM = GPT().to(config.device)\n",
        "    model_name = \"GPT-mini\"\n",
        "    LLM.load_state_dict(torch.load(f\"models/checkpoints/gpt/{model_name}.pth\", weights_only=True))\n",
        "    context = \"Who\"\n",
        "\n",
        "    context = torch.tensor([tokenizer.encode(context)], dtype=torch.long, device=config.device)\n",
        "    out = LLM.generate(context, config.inference_len)\n",
        "\n",
        "    if save:\n",
        "        path = config.save_dir + \"/GPT_out.txt\"\n",
        "        with open(path, 'w') as f:\n",
        "            f.write(tokenizer.decode(out.squeeze().tolist()))\n",
        "    else:\n",
        "        print(tokenizer.decode(out.squeeze().tolist()))\n",
        "\n",
        "# Generate text\n",
        "gpt_inference(save=False)"
      ],
      "metadata": {
        "id": "wd54DfiV2O17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Parameter Count"
      ],
      "metadata": {
        "id": "RGo_5RZE_rQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LLM = GPT()\n",
        "print(sum(p.numel() for p in LLM.parameters()))"
      ],
      "metadata": {
        "id": "QlmTmS-D9uyK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}