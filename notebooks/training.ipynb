{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s4xTmQKkrGR"
      },
      "source": [
        "# Language Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25b1VTmVkrGT",
        "outputId": "52f2107d-6754-4f7c-da58-81f775391e0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n",
            "/content/Self-GPT\n",
            "data  inference.py  models  notebooks  README.md  requirements.txt  training.py\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/inner-LMNt/Self-GPT.git\n",
        "%cd Self-GPT\n",
        "\n",
        "# Install requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "!pwd\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8efwRtXEkrGU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "from models.LLM.config import Config\n",
        "from models.LLM.GPT import GPT\n",
        "from models.LLM.bigram import BigramLanguageModel, BigramNoAttention\n",
        "from models.LLM.trigram import TrigramLanguageModel, TrigramNoAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5IjWv-4WkrGV"
      },
      "outputs": [],
      "source": [
        "def load_json(file):\n",
        "    with open(file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def n_gram(number=2):\n",
        "    config = Config()\n",
        "    # torch.manual_seed(config.seed)\n",
        "\n",
        "    model_name = \"shakespeare_model\"\n",
        "\n",
        "    if number == 2:\n",
        "        path = config.checkpoint_dir + f\"/bigram/{model_name}.pth\"\n",
        "        LLM = BigramLanguageModel().to(config.device)\n",
        "    else:\n",
        "        path = config.checkpoint_dir + f\"/trigram/{model_name}.pth\"\n",
        "        LLM = TrigramLanguageModel().to(config.device)\n",
        "\n",
        "    try:\n",
        "        LLM.load_state_dict(torch.load(path, map_location=config.device))\n",
        "        print(\"Model loaded successfully:\", path)\n",
        "        print(\"Training...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Training model from scratch...\")\n",
        "\n",
        "    data = load_json(config.data_dir + '/' + config.data_file)\n",
        "    data = torch.tensor(data, dtype=torch.long)\n",
        "\n",
        "    split = int(0.9 * len(data))\n",
        "    train_data, val_data = data[:split], data[split:]\n",
        "\n",
        "    def generate_batch(train):\n",
        "        data = train_data if train else val_data\n",
        "        offsets = torch.randint(len(data) - config.context_len, (config.batch_size,))\n",
        "        inputs = torch.stack([data[i:i+config.context_len] for i in offsets])\n",
        "        targets = torch.stack([data[i+1:i+config.context_len+1] for i in offsets])\n",
        "        return inputs.to(config.device), targets.to(config.device)\n",
        "\n",
        "    start = time.time()\n",
        "    optimizer = torch.optim.Adam(LLM.parameters(), lr=config.learning_rate)\n",
        "    for _ in range(10000):\n",
        "        x, y = generate_batch(True)\n",
        "        logits, loss = LLM.forward(x, y)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward() # Gradients\n",
        "        optimizer.step() # Update weights\n",
        "\n",
        "    print(\"\\n______________________________________________________\")\n",
        "    print(f\"Training loss: {loss.item()}, Validation loss: {LLM.forward(*generate_batch(False))[1].item()}\")\n",
        "    print(f\"Training time: {time.time() - start} seconds\")\n",
        "    print(\"______________________________________________________\\n\")\n",
        "\n",
        "    print(f\"Saving model to {path}\")\n",
        "    try:\n",
        "        torch.save(LLM.state_dict(), path)\n",
        "        print(\"Model saved successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyo_7Z1kkrGV"
      },
      "source": [
        "## Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVUUsLt9krGX",
        "outputId": "248f1d56-ae17-4ef0-eeb7-56f2ea8d44f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading model: [Errno 2] No such file or directory: './models/checkpoints/bigram/shakespeare_model.pth'\n",
            "Training model from scratch...\n",
            "\n",
            "______________________________________________________\n",
            "Training loss: 2.3169105052948, Validation loss: 2.2330212593078613\n",
            "Training time: 96.30599761009216 seconds\n",
            "______________________________________________________\n",
            "\n",
            "Saving model to ./models/checkpoints/bigram/shakespeare_model.pth\n",
            "Model saved successfully\n",
            "\n",
            "Finished.\n"
          ]
        }
      ],
      "source": [
        "n_gram(2) # or 3\n",
        "print(\"\\nFinished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cARXX-SVkrGX"
      },
      "source": [
        "## Download Trained Model\n",
        "\n",
        "After training, you can download the model using the code or just right-\n",
        "clicking the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GEi14tSikrGX",
        "outputId": "85afcaa9-2fea-47e7-e056-f167592ac688"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_87a1f968-f092-429f-834a-66d88cb26f3d\", \"shakespeare_model.pth\", 112888)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "config = Config()\n",
        "model_type = \"bigram\"\n",
        "model_name = \"shakespeare_model\"\n",
        "\n",
        "# Adjust the path based on which model you trained\n",
        "path = config.checkpoint_dir + f\"/{model_type}/{model_name}.pth\"\n",
        "\n",
        "if os.path.exists(path):\n",
        "    files.download(path)\n",
        "else:\n",
        "    print(f\"Model file not found at {path}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}